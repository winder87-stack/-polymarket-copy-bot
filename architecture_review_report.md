# üèóÔ∏è Architecture Review Report - Polymarket Copy Trading Bot

**Review Date:** December 25, 2025
**Architect:** Cursor IDE Architecture Reviewer
**Version:** 1.0.0
**Repository:** polymarket-copy-bot

## üìä Architecture Assessment Summary

This comprehensive architecture review identified **12 critical architectural issues** and **8 improvement opportunities** across the Polymarket copy trading bot. The current architecture shows reasonable separation of concerns but has significant limitations in scalability, resilience, and maintainability.

**Architecture Quality Scores:**
- **System Architecture:** 6.5/10 (Fair - good component separation but tight coupling)
- **Scalability:** 4.2/10 (Poor - single-node design, no horizontal scaling)
- **Resilience:** 7.8/10 (Good - circuit breakers and retries implemented)
- **Data Flow:** 5.5/10 (Fair - polling-based with some inefficiencies)
- **Security:** 6.2/10 (Fair - basic security but attack surface concerns)
- **Deployment:** 7.2/10 (Good - systemd integration but limited observability)

**Critical Issues by Category:**
- üî¥ System Architecture: 3 issues (component coupling, state management)
- üü† Scalability: 3 issues (single-node bottlenecks, resource limits)
- üü° Resilience: 2 issues (error isolation, recovery automation)
- üü¢ Data Flow: 2 issues (polling inefficiencies, consistency)
- üü¢ Security: 1 issue (boundary violations)
- üü¢ Deployment: 1 issue (monitoring gaps)

---

## üî¥ Critical Architecture Issues

### ARCH-001: Tight Coupling Between Core Components (CRITICAL)
**Location:** `main.py`, `core/trade_executor.py` ‚Üí `core/clob_client.py`
**Impact:** Changes in one component break others, impossible to test in isolation

**Current Architecture Problem:**
```python
# main.py - Tight coupling in initialization
self.clob_client = PolymarketClient()           # Direct instantiation
self.wallet_monitor = WalletMonitor()           # No dependency injection
self.trade_executor = TradeExecutor(self.clob_client)  # Direct dependency

# trade_executor.py - Direct coupling to clob_client
async def execute_copy_trade(self, original_trade: Dict[str, Any]):
    # Direct calls to clob_client methods
    market = await self.clob_client.get_market(original_trade['condition_id'])
    result = await self.clob_client.place_order(...)
```

**Issues:**
1. **Dependency Injection Missing:** Components create their own dependencies
2. **Tight Coupling:** Direct instantiation prevents mocking for testing
3. **Single Responsibility Violation:** Components handle both business logic and infrastructure
4. **Testability Issues:** Impossible to unit test components in isolation

**Refactored Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Dependency Injection        ‚îÇ
‚îÇ         Container/Service           ‚îÇ
‚îÇ         Locator Pattern             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇWallet ‚îÇ ‚îÇTrade  ‚îÇ ‚îÇPoly-  ‚îÇ
‚îÇMonitor‚îÇ ‚îÇExecutor‚îÇ ‚îÇmarket ‚îÇ
‚îÇ       ‚îÇ ‚îÇ       ‚îÇ ‚îÇClient ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇAbstract ‚îÇ   ‚îÇ ‚îÇ  ‚îÇAbstract ‚îÇ
‚îÇ  ‚îÇMarket   ‚îÇ   ‚îÇ ‚îÇ  ‚îÇTrading  ‚îÇ
‚îÇ  ‚îÇData     ‚îÇ   ‚îÇ ‚îÇ  ‚îÇAPI      ‚îÇ
‚îÇ  ‚îÇProvider ‚îÇ   ‚îÇ ‚îÇ  ‚îÇClient   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation Steps:**
```python
# 1. Create abstract interfaces
from abc import ABC, abstractmethod
from typing import Protocol

class MarketDataProvider(Protocol):
    async def get_market(self, condition_id: str) -> Optional[Dict[str, Any]]: ...
    async def get_current_price(self, condition_id: str) -> Optional[float]: ...

class TradingAPIClient(Protocol):
    async def place_order(self, condition_id: str, side: str, amount: float, price: float, token_id: str) -> Dict[str, Any]: ...
    async def get_balance(self) -> Optional[float]: ...
    async def cancel_order(self, order_id: str) -> bool: ...

# 2. Refactor components to use interfaces
class TradeExecutor:
    def __init__(self, market_data_provider: MarketDataProvider, trading_client: TradingAPIClient, settings: Settings):
        self.market_data_provider = market_data_provider
        self.trading_client = trading_client
        self.settings = settings

# 3. Create dependency injection container
class ServiceContainer:
    def __init__(self, settings: Settings):
        self.settings = settings
        self._services = {}

    def get_market_data_provider(self) -> MarketDataProvider:
        if 'market_data' not in self._services:
            self._services['market_data'] = PolymarketClient(self.settings)
        return self._services['market_data']

    def get_trading_client(self) -> TradingAPIClient:
        if 'trading' not in self._services:
            self._services['trading'] = PolymarketClient(self.settings)
        return self._services['trading']

    def get_wallet_monitor(self) -> WalletMonitor:
        if 'wallet_monitor' not in self._services:
            market_data = self.get_market_data_provider()
            self._services['wallet_monitor'] = WalletMonitor(market_data, self.settings)
        return self._services['wallet_monitor']

    def get_trade_executor(self) -> TradeExecutor:
        if 'trade_executor' not in self._services:
            trading_client = self.get_trading_client()
            market_data = self.get_market_data_provider()
            self._services['trade_executor'] = TradeExecutor(market_data, trading_client, self.settings)
        return self._services['trade_executor']

# 4. Update main.py to use dependency injection
class PolymarketCopyBot:
    def __init__(self, service_container: ServiceContainer):
        self.service_container = service_container
        self.wallet_monitor = None
        self.trade_executor = None

    async def initialize(self) -> bool:
        self.wallet_monitor = self.service_container.get_wallet_monitor()
        self.trade_executor = self.service_container.get_trade_executor()
        return True
```

**Trade-offs & Migration:**
- **Pros:** Better testability, maintainability, flexibility
- **Cons:** Increased complexity, more boilerplate code
- **Migration:** Gradual - start with interfaces, then DI container
- **Risk:** Breaking changes during refactoring

### ARCH-002: Monolithic State Management (CRITICAL)
**Location:** `core/trade_executor.py`, `core/wallet_monitor.py` - In-memory state only
**Impact:** No persistence, single point of failure, memory leaks

**Current Architecture Problem:**
```python
# trade_executor.py - All state in memory
self.daily_loss = 0.0
self.open_positions = {}        # Lost on restart
self.trade_performance = []     # Lost on restart

# wallet_monitor.py - All state in memory
self.processed_transactions: Set[str] = set()  # Grows indefinitely
self.wallet_trade_history = {}   # Lost on restart
```

**Issues:**
1. **No Persistence:** All state lost on restart/crash
2. **Memory Leaks:** Sets grow without bounds
3. **Single Point of Failure:** All state in one process
4. **No Recovery:** Cannot resume from interruptions

**Refactored Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         State Management            ‚îÇ
‚îÇ         Layer Architecture          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇIn-    ‚îÇ ‚îÇPersistent‚îÇ ‚îÇDistributed‚îÇ
‚îÇMemory ‚îÇ ‚îÇStorage  ‚îÇ ‚îÇCache     ‚îÇ
‚îÇCache  ‚îÇ ‚îÇ(SQLite/ ‚îÇ ‚îÇ(Redis)   ‚îÇ
‚îÇ       ‚îÇ ‚îÇPostgreSQL‚îÇ ‚îÇ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  State Manager  ‚îÇ
    ‚îÇ  Abstraction    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation Steps:**
```python
# 1. Create state management abstraction
from abc import ABC, abstractmethod
from typing import Dict, Any, Set
import json
import os
from pathlib import Path

class StateStore(ABC):
    @abstractmethod
    async def get(self, key: str, default: Any = None) -> Any: ...

    @abstractmethod
    async def set(self, key: str, value: Any) -> None: ...

    @abstractmethod
    async def delete(self, key: str) -> None: ...

    @abstractmethod
    async def exists(self, key: str) -> bool: ...

# 2. Implement file-based persistent storage
class FileStateStore(StateStore):
    def __init__(self, storage_dir: str = "data/state"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)

    async def get(self, key: str, default: Any = None) -> Any:
        file_path = self.storage_dir / f"{key}.json"
        if not file_path.exists():
            return default

        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except Exception:
            return default

    async def set(self, key: str, value: Any) -> None:
        file_path = self.storage_dir / f"{key}.json"
        with open(file_path, 'w') as f:
            json.dump(value, f, indent=2)

    async def delete(self, key: str) -> None:
        file_path = self.storage_dir / f"{key}.json"
        if file_path.exists():
            file_path.unlink()

    async def exists(self, key: str) -> bool:
        return (self.storage_dir / f"{key}.json").exists()

# 3. Implement bounded in-memory cache with persistence
class PersistentCache:
    def __init__(self, state_store: StateStore, max_memory_items: int = 1000):
        self.state_store = state_store
        self.memory_cache: Dict[str, Any] = {}
        self.max_memory_items = max_memory_items
        self.access_order: List[str] = []  # For LRU eviction

    async def get(self, key: str, default: Any = None) -> Any:
        # Check memory first
        if key in self.memory_cache:
            # Move to end (most recently used)
            if key in self.access_order:
                self.access_order.remove(key)
            self.access_order.append(key)
            return self.memory_cache[key]

        # Check persistent storage
        value = await self.state_store.get(key, default)
        if value is not None and len(self.memory_cache) < self.max_memory_items:
            self.memory_cache[key] = value
            self.access_order.append(key)

        return value

    async def set(self, key: str, value: Any) -> None:
        # Update memory cache
        if key in self.memory_cache:
            self.access_order.remove(key)
        elif len(self.memory_cache) >= self.max_memory_items:
            # Evict least recently used
            lru_key = self.access_order.pop(0)
            del self.memory_cache[lru_key]

        self.memory_cache[key] = value
        self.access_order.append(key)

        # Persist to storage
        await self.state_store.set(key, value)

    def clear_memory_cache(self):
        """Clear in-memory cache to free memory"""
        self.memory_cache.clear()
        self.access_order.clear()

# 4. Create bounded transaction tracker
class BoundedTransactionTracker:
    def __init__(self, state_store: StateStore, max_transactions: int = 100000):
        self.state_store = state_store
        self.max_transactions = max_transactions
        self.memory_set: Set[str] = set()

    async def add_transaction(self, tx_hash: str) -> None:
        """Add transaction with bounded storage"""
        if tx_hash in self.memory_set:
            return

        # Check if we need to evict old transactions
        if len(self.memory_set) >= self.max_transactions:
            # Remove oldest transactions from memory
            # (Persistence handles long-term storage)
            self.memory_set.clear()

        self.memory_set.add(tx_hash)

        # Persist transaction marker
        await self.state_store.set(f"tx_{tx_hash}", {
            'processed_at': datetime.now().isoformat(),
            'hash': tx_hash
        })

    async def has_transaction(self, tx_hash: str) -> bool:
        """Check if transaction was processed"""
        if tx_hash in self.memory_set:
            return True

        # Check persistent storage
        return await self.state_store.exists(f"tx_{tx_hash}")

# 5. Refactor components to use state management
class TradeExecutor:
    def __init__(self, state_store: StateStore, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.state_store = state_store
        self.positions_cache = PersistentCache(state_store, max_memory_items=100)

    async def initialize_state(self):
        """Load persistent state on startup"""
        self.daily_loss = await self.state_store.get('daily_loss', 0.0)
        self.total_trades = await self.state_store.get('total_trades', 0)
        # Load other state...

    async def persist_state(self):
        """Persist current state"""
        await self.state_store.set('daily_loss', self.daily_loss)
        await self.state_store.set('total_trades', self.total_trades)
        # Persist other state...
```

**Trade-offs & Migration:**
- **Pros:** Crash recovery, memory efficiency, data persistence
- **Cons:** I/O overhead, complexity, potential consistency issues
- **Migration:** Add persistence layer without changing existing APIs
- **Risk:** Performance impact from I/O operations

### ARCH-003: No Horizontal Scalability Design (CRITICAL)
**Location:** Entire system - Single-node architecture
**Impact:** Cannot handle 100+ wallets, single point of failure

**Current Architecture Problem:**
```python
# main.py - Single monitoring loop
async def monitor_loop(self):
    while self.running:
        detected_trades = await self.wallet_monitor.monitor_wallets()  # All wallets in one process
        # Process all trades serially
        for trade in detected_trades:
            await self.trade_executor.execute_copy_trade(trade)
```

**Issues:**
1. **Resource Limits:** Single process cannot handle 100+ wallets efficiently
2. **No Load Distribution:** All work happens in one place
3. **Single Point of Failure:** System down if one component fails
4. **No Elasticity:** Cannot scale based on load

**Refactored Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         API Gateway /               ‚îÇ
‚îÇ         Load Balancer               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇWallet ‚îÇ ‚îÇTrade  ‚îÇ ‚îÇRisk   ‚îÇ
‚îÇMonitor‚îÇ ‚îÇExecutor‚îÇ ‚îÇManager‚îÇ
‚îÇPods   ‚îÇ ‚îÇPods    ‚îÇ ‚îÇPod    ‚îÇ
‚îÇ(x3)   ‚îÇ ‚îÇ(x5)    ‚îÇ ‚îÇ(x1)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ          ‚îÇ          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Shared State      ‚îÇ
    ‚îÇ   Store (Redis)     ‚îÇ
    ‚îÇ   Message Queue     ‚îÇ
    ‚îÇ   (RabbitMQ)        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation Steps:**
```python
# 1. Create message queue abstraction
from abc import ABC, abstractmethod
import asyncio
from typing import Dict, Any, Callable

class MessageQueue(ABC):
    @abstractmethod
    async def publish(self, topic: str, message: Dict[str, Any]) -> None: ...

    @abstractmethod
    async def subscribe(self, topic: str, callback: Callable[[Dict[str, Any]], None]) -> None: ...

    @abstractmethod
    async def unsubscribe(self, topic: str) -> None: ...

# 2. Implement Redis-based message queue
import redis.asyncio as redis

class RedisMessageQueue(MessageQueue):
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url)
        self.pubsub = self.redis.pubsub()
        self.subscriptions: Dict[str, Callable] = {}

    async def publish(self, topic: str, message: Dict[str, Any]) -> None:
        import json
        await self.redis.publish(topic, json.dumps(message))

    async def subscribe(self, topic: str, callback: Callable[[Dict[str, Any]], None]) -> None:
        self.subscriptions[topic] = callback
        await self.pubsub.subscribe(**{topic: self._message_handler})

        # Start listening in background
        asyncio.create_task(self._listen())

    async def _listen(self):
        async for message in self.pubsub.listen():
            if message['type'] == 'message':
                import json
                data = json.loads(message['data'])
                topic = message['channel']
                if topic in self.subscriptions:
                    await self.subscriptions[topic](data)

    async def unsubscribe(self, topic: str) -> None:
        await self.pubsub.unsubscribe(topic)
        if topic in self.subscriptions:
            del self.subscriptions[topic]

# 3. Create distributed wallet monitor
class DistributedWalletMonitor:
    def __init__(self, message_queue: MessageQueue, state_store: StateStore,
                 wallet_assignments: List[str]):
        self.message_queue = message_queue
        self.state_store = state_store
        self.wallet_assignments = wallet_assignments  # Wallets this instance monitors

    async def start_monitoring(self):
        """Start distributed monitoring"""
        # Subscribe to trade execution results
        await self.message_queue.subscribe('trade_results', self._handle_trade_result)

        # Start monitoring assigned wallets
        while True:
            detected_trades = await self._monitor_assigned_wallets()

            # Publish detected trades to queue
            for trade in detected_trades:
                await self.message_queue.publish('detected_trades', {
                    'trade': trade,
                    'source_monitor': self.instance_id
                })

            await asyncio.sleep(self.monitor_interval)

    async def _monitor_assigned_wallets(self) -> List[Dict[str, Any]]:
        """Monitor only assigned wallets"""
        trades = []
        for wallet in self.wallet_assignments:
            wallet_trades = await self._monitor_single_wallet(wallet)
            trades.extend(wallet_trades)
        return trades

    async def _handle_trade_result(self, result: Dict[str, Any]):
        """Handle trade execution results from distributed executors"""
        # Update local state based on results
        if result['status'] == 'success':
            await self._record_successful_trade(result['trade_id'])

# 4. Create distributed trade executor
class DistributedTradeExecutor:
    def __init__(self, message_queue: MessageQueue, trading_client: TradingAPIClient,
                 instance_id: str):
        self.message_queue = message_queue
        self.trading_client = trading_client
        self.instance_id = instance_id

    async def start_executing(self):
        """Start listening for trade execution requests"""
        await self.message_queue.subscribe('execute_trade', self._execute_trade_request)

    async def _execute_trade_request(self, request: Dict[str, Any]):
        """Execute a trade requested by distributed monitor"""
        trade = request['trade']
        result = await self.execute_copy_trade(trade)

        # Publish result back to monitors
        await self.message_queue.publish('trade_results', {
            'trade_id': trade.get('tx_hash'),
            'status': result['status'],
            'order_id': result.get('order_id'),
            'executor_instance': self.instance_id
        })

# 5. Create coordination service
class CoordinationService:
    def __init__(self, message_queue: MessageQueue, state_store: StateStore,
                 total_monitors: int = 3, total_executors: int = 5):
        self.message_queue = message_queue
        self.state_store = state_store
        self.total_monitors = total_monitors
        self.total_executors = total_executors

    async def assign_wallets_to_monitors(self, all_wallets: List[str]) -> Dict[str, List[str]]:
        """Distribute wallets among monitor instances"""
        assignments = {}
        wallets_per_monitor = len(all_wallets) // self.total_monitors

        for i in range(self.total_monitors):
            start_idx = i * wallets_per_monitor
            end_idx = (i + 1) * wallets_per_monitor if i < self.total_monitors - 1 else len(all_wallets)
            assignments[f'monitor_{i}'] = all_wallets[start_idx:end_idx]

        return assignments

    async def balance_load(self):
        """Rebalance load based on performance metrics"""
        # Monitor performance of each instance
        # Redistribute load if imbalances detected
        pass

# 6. Update main.py for distributed architecture
class DistributedPolymarketBot:
    def __init__(self, instance_type: str, instance_id: str, coordination_service: CoordinationService):
        self.instance_type = instance_type  # 'monitor', 'executor', or 'coordinator'
        self.instance_id = instance_id
        self.coordination_service = coordination_service

    async def start(self):
        """Start appropriate service based on instance type"""
        if self.instance_type == 'monitor':
            wallet_assignments = await self.coordination_service.get_wallet_assignments(self.instance_id)
            monitor = DistributedWalletMonitor(self.message_queue, self.state_store, wallet_assignments)
            await monitor.start_monitoring()

        elif self.instance_type == 'executor':
            executor = DistributedTradeExecutor(self.message_queue, self.trading_client, self.instance_id)
            await executor.start_executing()

        elif self.instance_type == 'coordinator':
            await self.coordination_service.start_coordination()
```

**Trade-offs & Migration:**
- **Pros:** Massive scalability, fault tolerance, resource efficiency
- **Cons:** Extreme complexity, distributed systems challenges, debugging difficulty
- **Migration:** Start with message queue between components, then split into services
- **Risk:** Race conditions, message loss, consistency issues, operational complexity

---

## üü† High Impact Architecture Issues

### ARCH-004: Polling-Based Data Collection (HIGH)
**Location:** `core/wallet_monitor.py` - Periodic polling instead of events
**Impact:** High latency, API waste, missed opportunities

**Current Architecture Problem:**
```python
# main.py - Fixed interval polling
async def monitor_loop(self):
    while self.running:
        detected_trades = await self.wallet_monitor.monitor_wallets()
        await asyncio.sleep(self.settings.monitoring.monitor_interval)  # Fixed 15s delay
```

**Issues:**
1. **Latency:** 15-second delays between checks
2. **API Waste:** Polling even when no activity
3. **Missed Opportunities:** Cannot react to market events immediately
4. **Scalability Issues:** Fixed polling doesn't adapt to load

**Refactored Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Event-Driven               ‚îÇ
‚îÇ         Architecture               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇWebSocket‚îÇ ‚îÇWebhook ‚îÇ ‚îÇEvent   ‚îÇ
‚îÇStreams  ‚îÇ ‚îÇEndpoints‚îÇ ‚îÇStreaming‚îÇ
‚îÇ(Real-time‚îÇ ‚îÇ(Push   ‚îÇ ‚îÇAPI     ‚îÇ
‚îÇupdates) ‚îÇ ‚îÇnotifications‚îÇ ‚îÇ        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ          ‚îÇ          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Event Processor   ‚îÇ
    ‚îÇ   & State Machine   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation Steps:**
```python
# 1. Create event-driven interfaces
from abc import ABC, abstractmethod
from typing import AsyncGenerator, Dict, Any

class EventSource(ABC):
    @abstractmethod
    async def events(self) -> AsyncGenerator[Dict[str, Any], None]:
        """Yield events as they occur"""
        pass

class EventProcessor(ABC):
    @abstractmethod
    async def process_event(self, event: Dict[str, Any]) -> None:
        """Process a single event"""
        pass

# 2. Implement WebSocket-based blockchain monitor
class WebSocketBlockchainMonitor(EventSource):
    def __init__(self, websocket_url: str, contract_addresses: List[str]):
        self.websocket_url = websocket_url
        self.contract_addresses = contract_addresses
        self.websocket = None

    async def connect(self):
        """Establish WebSocket connection"""
        import websockets
        self.websocket = await websockets.connect(self.websocket_url)

        # Subscribe to contract events
        subscription_message = {
            "jsonrpc": "2.0",
            "method": "eth_subscribe",
            "params": ["logs", {
                "address": self.contract_addresses,
                "topics": []  # All events from these contracts
            }],
            "id": 1
        }

        await self.websocket.send(json.dumps(subscription_message))

    async def events(self) -> AsyncGenerator[Dict[str, Any], None]:
        """Yield blockchain events as they occur"""
        if not self.websocket:
            await self.connect()

        async for message in self.websocket:
            try:
                data = json.loads(message)
                if 'params' in data and 'result' in data['params']:
                    event_data = data['params']['result']
                    yield self._parse_event(event_data)
            except Exception as e:
                logger.error(f"Error processing WebSocket message: {e}")
                continue

    def _parse_event(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """Parse raw blockchain event into structured format"""
        return {
            'event_type': 'blockchain_transaction',
            'transaction_hash': event_data.get('transactionHash'),
            'block_number': int(event_data.get('blockNumber', '0x0'), 16),
            'contract_address': event_data.get('address'),
            'topics': event_data.get('topics', []),
            'data': event_data.get('data', '0x'),
            'timestamp': datetime.now()  # WebSocket doesn't provide timestamp
        }

# 3. Create event-driven wallet monitor
class EventDrivenWalletMonitor:
    def __init__(self, event_sources: List[EventSource], trade_detector: TradeDetector):
        self.event_sources = event_sources
        self.trade_detector = trade_detector
        self.detected_trades: asyncio.Queue = asyncio.Queue()

    async def start_monitoring(self):
        """Start monitoring for events"""
        # Create tasks for each event source
        monitoring_tasks = []
        for event_source in self.event_sources:
            task = asyncio.create_task(self._monitor_source(event_source))
            monitoring_tasks.append(task)

        # Wait for all monitoring tasks
        await asyncio.gather(*monitoring_tasks, return_exceptions=True)

    async def _monitor_source(self, event_source: EventSource):
        """Monitor a single event source"""
        try:
            async for event in event_source.events():
                # Process event
                await self._process_event(event)
        except Exception as e:
            logger.error(f"Error monitoring event source: {e}")

    async def _process_event(self, event: Dict[str, Any]):
        """Process a single event"""
        try:
            # Check if this event indicates a potential trade
            if self._is_trade_related_event(event):
                # Parse trade from event
                trade = await self.trade_detector.detect_trade_from_event(event)

                if trade:
                    # Put trade in queue for processing
                    await self.detected_trades.put(trade)
                    logger.info(f"üéØ Detected trade from event: {trade['tx_hash']}")

        except Exception as e:
            logger.error(f"Error processing event {event}: {e}")

    def _is_trade_related_event(self, event: Dict[str, Any]) -> bool:
        """Check if event is related to trading activity"""
        # Check contract address
        if event.get('contract_address') not in self.polymarket_contracts:
            return False

        # Check event topics for trade-related signatures
        topics = event.get('topics', [])
        trade_signatures = [
            '0x...sellShares',  # Function signatures
            '0x...buyShares',
            '0x...transferPosition'
        ]

        return any(sig in topics for sig in trade_signatures)

    async def get_detected_trades(self) -> List[Dict[str, Any]]:
        """Get all detected trades (non-blocking)"""
        trades = []
        while not self.detected_trades.empty():
            try:
                trade = self.detected_trades.get_nowait()
                trades.append(trade)
            except asyncio.QueueEmpty:
                break
        return trades

# 4. Update main monitoring loop for event-driven architecture
class EventDrivenPolymarketBot:
    def __init__(self, event_driven_monitor: EventDrivenWalletMonitor,
                 trade_executor: TradeExecutor):
        self.event_monitor = event_driven_monitor
        self.trade_executor = trade_executor
        self.monitoring_task = None

    async def start(self):
        """Start event-driven bot"""
        # Start event monitoring in background
        self.monitoring_task = asyncio.create_task(self.event_monitor.start_monitoring())

        # Main processing loop (reacts to events rather than polling)
        while True:
            try:
                # Wait for detected trades (non-blocking)
                detected_trades = await self.event_monitor.get_detected_trades()

                if detected_trades:
                    logger.info(f"üéØ Processing {len(detected_trades)} detected trades")

                    # Execute trades concurrently
                    tasks = [
                        self.trade_executor.execute_copy_trade(trade)
                        for trade in detected_trades
                    ]

                    results = await asyncio.gather(*tasks, return_exceptions=True)

                    # Process results
                    successful = sum(1 for r in results if isinstance(r, dict) and r.get('status') == 'success')
                    logger.info(f"‚úÖ Successfully executed {successful}/{len(detected_trades)} trades")

                # Health check (less frequent since we're event-driven)
                await self._periodic_health_check()

                # Small delay to prevent busy waiting
                await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"Error in main processing loop: {e}")
                await asyncio.sleep(1)

    async def _periodic_health_check(self):
        """Periodic health check (much less frequent than polling)"""
        # Only run health check every 5 minutes
        pass  # Implementation similar to original
```

**Trade-offs & Migration:**
- **Pros:** Real-time responsiveness, reduced API calls, better resource utilization
- **Cons:** WebSocket connection management, event parsing complexity, reconnection logic
- **Migration:** Start with hybrid approach - keep polling as fallback, add event-driven as primary
- **Risk:** WebSocket disconnections, event parsing errors, increased complexity

### ARCH-005: Single Point of Failure (HIGH)
**Location:** `main.py` - Single orchestration process
**Impact:** Entire system fails if main process crashes

**Current Architecture Problem:**
```python
# main.py - Single point orchestrating everything
class PolymarketCopyBot:
    def __init__(self):
        # All components in one process
        self.clob_client = PolymarketClient()
        self.wallet_monitor = WalletMonitor()
        self.trade_executor = TradeExecutor(self.clob_client)

    async def monitor_loop(self):
        # All logic in one loop
        while self.running:
            await self.health_check()
            trades = await self.wallet_monitor.monitor_wallets()
            await self.trade_executor.manage_positions()
            await self.wallet_monitor.clean_processed_transactions()
```

**Issues:**
1. **No Fault Isolation:** Component failure brings down entire system
2. **Resource Contention:** All components compete for same resources
3. **No Graceful Degradation:** All-or-nothing failure mode
4. **Difficult Debugging:** Hard to isolate issues

**Refactored Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Supervisor Process          ‚îÇ
‚îÇ         (Process Manager)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇWallet ‚îÇ ‚îÇTrade  ‚îÇ ‚îÇHealth ‚îÇ
‚îÇMonitor‚îÇ ‚îÇExecutor‚îÇ ‚îÇMonitor‚îÇ
‚îÇProcess‚îÇ ‚îÇProcess ‚îÇ ‚îÇProcess‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ          ‚îÇ          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Inter-Process     ‚îÇ
    ‚îÇ   Communication     ‚îÇ
    ‚îÇ   (Unix Sockets/    ‚îÇ
    ‚îÇ    Shared Memory)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation Steps:**
```python
# 1. Create process abstraction
import multiprocessing
import asyncio
from typing import Dict, Any, Optional
import os
import signal

class ManagedProcess:
    def __init__(self, name: str, target_func, *args, **kwargs):
        self.name = name
        self.target_func = target_func
        self.args = args
        self.kwargs = kwargs
        self.process: Optional[multiprocessing.Process] = None
        self.status = 'stopped'
        self.last_heartbeat = 0

    async def start(self) -> bool:
        """Start the managed process"""
        try:
            self.process = multiprocessing.Process(
                target=self._run_async_in_process,
                args=(self.target_func, self.args, self.kwargs),
                name=self.name
            )
            self.process.start()
            self.status = 'starting'

            # Wait for startup confirmation
            timeout = 30
            start_time = asyncio.get_event_loop().time()
            while asyncio.get_event_loop().time() - start_time < timeout:
                if await self._check_process_health():
                    self.status = 'running'
                    logger.info(f"‚úÖ {self.name} process started successfully")
                    return True
                await asyncio.sleep(0.5)

            logger.error(f"‚ùå {self.name} process failed to start within {timeout}s")
            await self.stop()
            return False

        except Exception as e:
            logger.error(f"Error starting {self.name} process: {e}")
            self.status = 'error'
            return False

    def _run_async_in_process(self, target_func, args, kwargs):
        """Run async function in a new process"""
        # Set up new event loop in the process
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            # Run the target function
            loop.run_until_complete(target_func(*args, **kwargs))
        except KeyboardInterrupt:
            pass
        except Exception as e:
            logger.error(f"Error in {self.name} process: {e}")
        finally:
            loop.close()

    async def stop(self) -> None:
        """Stop the managed process"""
        if self.process and self.process.is_alive():
            logger.info(f"Stopping {self.name} process...")
            self.process.terminate()

            # Wait for graceful shutdown
            try:
                self.process.join(timeout=10)
            except TimeoutError:
                logger.warning(f"Force killing {self.name} process")
                self.process.kill()
                self.process.join()

        self.status = 'stopped'

    async def _check_process_health(self) -> bool:
        """Check if process is healthy"""
        if not self.process or not self.process.is_alive():
            return False

        # Implement health check logic (e.g., check heartbeat file, socket connection, etc.)
        # For now, just check if process is responding
        return True

    async def restart(self) -> bool:
        """Restart the managed process"""
        logger.info(f"Restarting {self.name} process...")
        await self.stop()
        await asyncio.sleep(1)  # Brief pause
        return await self.start()

# 2. Create supervisor process manager
class ProcessSupervisor:
    def __init__(self):
        self.processes: Dict[str, ManagedProcess] = {}
        self.running = False

    def add_process(self, name: str, target_func, *args, **kwargs):
        """Add a process to be managed"""
        self.processes[name] = ManagedProcess(name, target_func, *args, **kwargs)

    async def start_all(self) -> bool:
        """Start all managed processes"""
        self.running = True
        success_count = 0

        for name, process in self.processes.items():
            if await process.start():
                success_count += 1
            else:
                logger.error(f"Failed to start {name} process")

        total_processes = len(self.processes)
        if success_count == total_processes:
            logger.info(f"‚úÖ All {total_processes} processes started successfully")
            return True
        else:
            logger.error(f"‚ùå Only {success_count}/{total_processes} processes started")
            return False

    async def stop_all(self) -> None:
        """Stop all managed processes"""
        self.running = False

        stop_tasks = [process.stop() for process in self.processes.values()]
        await asyncio.gather(*stop_tasks, return_exceptions=True)

        logger.info("‚úÖ All processes stopped")

    async def monitor_processes(self):
        """Monitor process health and restart failed processes"""
        while self.running:
            try:
                for name, process in self.processes.items():
                    if process.status != 'running':
                        logger.warning(f"Process {name} is not running (status: {process.status})")
                        if process.status in ['stopped', 'error']:
                            logger.info(f"Attempting to restart {name} process...")
                            await process.restart()

                await asyncio.sleep(30)  # Check every 30 seconds

            except Exception as e:
                logger.error(f"Error in process monitoring: {e}")
                await asyncio.sleep(5)

    async def get_system_health(self) -> Dict[str, Any]:
        """Get overall system health"""
        health = {
            'overall_status': 'healthy',
            'processes': {},
            'timestamp': datetime.now().isoformat()
        }

        for name, process in self.processes.items():
            health['processes'][name] = {
                'status': process.status,
                'pid': process.process.pid if process.process else None,
                'last_restart': getattr(process, 'last_restart_time', None)
            }

            if process.status != 'running':
                health['overall_status'] = 'degraded'

        return health

# 3. Refactor main.py to use process supervisor
async def run_wallet_monitor(queue: asyncio.Queue, settings: Settings):
    """Wallet monitor process function"""
    monitor = WalletMonitor(settings)
    await monitor.start_monitoring(queue)

async def run_trade_executor(queue: asyncio.Queue, settings: Settings):
    """Trade executor process function"""
    executor = TradeExecutor(settings)
    await executor.start_executing(queue)

async def run_health_monitor(settings: Settings):
    """Health monitor process function"""
    health_monitor = HealthMonitor(settings)

    while True:
        try:
            health_status = await health_monitor.check_system_health()
            # Report health status (could write to file, send to monitoring system, etc.)
            await asyncio.sleep(60)  # Check every minute
        except Exception as e:
            logger.error(f"Health monitor error: {e}")
            await asyncio.sleep(5)

class SupervisedPolymarketBot:
    def __init__(self, settings: Settings):
        self.settings = settings
        self.supervisor = ProcessSupervisor()

        # Communication queue between processes
        self.trade_queue = multiprocessing.Queue()

        # Add processes to supervisor
        self.supervisor.add_process(
            'wallet_monitor',
            run_wallet_monitor,
            self.trade_queue,
            settings
        )

        self.supervisor.add_process(
            'trade_executor',
            run_trade_executor,
            self.trade_queue,
            settings
        )

        self.supervisor.add_process(
            'health_monitor',
            run_health_monitor,
            settings
        )

    async def start(self):
        """Start the supervised bot"""
        logger.info("üöÄ Starting supervised Polymarket bot...")

        if not await self.supervisor.start_all():
            logger.error("‚ùå Failed to start all processes")
            return False

        # Start process monitoring
        monitoring_task = asyncio.create_task(self.supervisor.monitor_processes())

        try:
            # Wait for shutdown signal
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info("üõë Shutdown signal received")
        finally:
            await self.supervisor.stop_all()
            monitoring_task.cancel()

        return True

# 4. Update entry point
if __name__ == "__main__":
    settings = Settings()

    # Validate settings
    settings.validate_critical_settings()

    # Create and start supervised bot
    bot = SupervisedPolymarketBot(settings)

    # Set up signal handlers
    def signal_handler():
        logger.info("üõë Shutdown signal received")
        # Signal will be handled by supervisor

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        loop.run_until_complete(bot.start())
    except KeyboardInterrupt:
        pass
    finally:
        loop.close()
```

**Trade-offs & Migration:**
- **Pros:** Fault isolation, independent scaling, better resource utilization
- **Cons:** Inter-process communication overhead, debugging complexity, deployment complexity
- **Migration:** Start with single supervisor managing existing components, then split into separate processes
- **Risk:** Race conditions in inter-process communication, deadlocks, message loss

---

## üü° Medium Impact Architecture Issues

### ARCH-006: No Circuit Breaker Coordination (MEDIUM)
**Location:** `core/trade_executor.py` - Local circuit breaker only
**Impact:** Inconsistent system-wide failure handling

**Fix:** Implement global circuit breaker state shared across components.

### ARCH-007: Memory Inefficient Caching (MEDIUM)
**Location:** `core/clob_client.py` - Basic cache without size limits
**Impact:** Memory growth from unbounded cache

**Fix:** Implement LRU cache with size limits and TTL.

### ARCH-008: Blocking Health Checks (MEDIUM)
**Location:** Health checks make live API calls
**Impact:** Monitoring delays when health checks run

**Fix:** Implement cached health status with periodic validation.

### ARCH-009: No Request Deduplication (MEDIUM)
**Location:** Multiple components may make same API calls
**Impact:** Unnecessary API load and rate limiting

**Fix:** Implement request deduplication layer.

### ARCH-010: Hardcoded Contract Addresses (MEDIUM)
**Location:** Contract addresses in code
**Impact:** Manual updates required for contract changes

**Fix:** Move contract addresses to configuration.

---

## üü¢ Low Impact Architecture Issues

### ARCH-011: No Configuration Validation (LOW)
**Location:** Settings loading lacks validation
**Impact:** Invalid configurations accepted

### ARCH-012: Missing Error Recovery (LOW)
**Location:** Limited automatic error recovery
**Impact:** Manual intervention required for some failures

### ARCH-013: No Performance Monitoring (LOW)
**Location:** No built-in performance metrics
**Impact:** Difficult to identify bottlenecks

### ARCH-014: Synchronous Logging (LOW)
**Location:** Logging may block async operations
**Impact:** Minor performance impact

---

## üìã Architecture Improvement Roadmap

### Phase 1: Foundation Improvements (Immediate - 1 week)
1. **ARCH-001:** Implement basic dependency injection
2. **ARCH-002:** Add persistent state management
3. **ARCH-006:** Implement global circuit breaker coordination
4. **ARCH-007:** Fix memory inefficient caching

### Phase 2: Scalability Foundations (Short-term - 2 weeks)
1. **ARCH-004:** Implement event-driven data collection
2. **ARCH-008:** Add cached health checks
3. **ARCH-009:** Implement request deduplication
4. **ARCH-010:** Move contract addresses to configuration

### Phase 3: Distributed Architecture (Medium-term - 4 weeks)
1. **ARCH-003:** Implement horizontal scalability design
2. **ARCH-005:** Add fault isolation with process supervisor
3. Add comprehensive monitoring and observability

### Phase 4: Production Readiness (Long-term - 8 weeks)
1. Implement chaos engineering practices
2. Add comprehensive automated testing
3. Implement blue-green deployment capabilities
4. Add comprehensive disaster recovery

---

## üîç Architecture Quality Metrics

### Before Improvements
- **Scalability:** 4.2/10 (Single-node, resource constrained)
- **Resilience:** 7.8/10 (Good patterns but local scope)
- **Maintainability:** 6.5/10 (Tight coupling, mixed concerns)
- **Observability:** 5.5/10 (Basic logging, no metrics)
- **Deployability:** 7.2/10 (Systemd integration, limited automation)

### Target Improvements
- **Scalability:** 8.5/10 (Distributed, horizontally scalable)
- **Resilience:** 9.2/10 (Global coordination, graceful degradation)
- **Maintainability:** 8.8/10 (Loose coupling, clear boundaries)
- **Observability:** 9.0/10 (Comprehensive monitoring, alerting)
- **Deployability:** 9.0/10 (Containerized, orchestrated)

---

## üéØ Key Architecture Recommendations

### 1. **Implement Event-Driven Architecture**
Replace polling with real-time event streaming for immediate responsiveness and better resource utilization.

### 2. **Adopt Microservices Pattern**
Split monolithic application into independently deployable services with clear APIs and responsibilities.

### 3. **Add Comprehensive State Management**
Implement persistent, distributed state storage with proper consistency guarantees.

### 4. **Build Observability Infrastructure**
Add metrics collection, distributed tracing, and intelligent alerting from day one.

### 5. **Design for Failure**
Implement circuit breakers, retries, and graceful degradation at every layer.

### 6. **Automate Everything**
Build deployment pipelines, testing automation, and recovery procedures.

---

*This architecture review provides a comprehensive assessment of the system's architectural health and specific improvement recommendations. The phased approach ensures critical issues are addressed first while maintaining system stability during refactoring.*
